# 记录一次排查OOM的经历

## 一、背景

最近新服务上线，运行了一段时间都很平稳，没有出现什么大的异常，突然有一天运维同事通知说注册中心上服务掉线了。于是登录了发生异常服务的组件，查看日志信息，关键信息如图

![image-20240714205652825](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/image-20240714205652825.png)

![image-20240714205739132](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/image-20240714205739132.png)

 从上面两个图片可以简单了解到：

1、服务发生了OOM异常
2、Consul链接因为Connection pool shut down而链接失败。

## 二、问题分析

### 猜想1：是不是因为Consul服务宕机，导致服务链接不上，并且短时间重试多次造成OOM?
实际：整个错误持续了几个小时，consul服务宕机其他服务也应该收到影响，但是其他服务并没有问题。所以猜想1应该不对。

### 猜想2：Connection pool shut down 这个问题是因为什么产生的？是不是可以从该地方入手，根据图中的异常位置可以查看代码。

查看对应的

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192013061-341158237.png)

继续查看：

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192026001-683565161.png)

可以判断 isShutDown的字段被设置为了true，才会导致打印异常信息。

继续全局搜索，查找shutdown可能赋值为true的地方：

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192043919-1845050273.png)

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192053952-1877733318.png)

最终定位到能够调用shutdown只有finalize()或者close()。

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192102480-944293073.png)

所以猜想一定是发生OOM之后，GC在回收内存的时候调用了finalize()方法，继而引发的isShutDown变为true。

### 猜想3：我们是否可以通过dump文件，查看内存泄漏的地方，进而在本地模拟这个场景。

找到运维下载OOM时候的dump文件，经过MAT工具一番查看，可以很明显的找到内存泄漏的点。

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192127251-1503038222.png)

如图所示：据dump文件描述，PoolingHttpClientConnectionManager 这个对象居然占到了整个大对象内存的90%多，并且是被一个数组对象引用着的，引用的对象数量居然有60万。继续查看详细：可以看到这个对象应该是在 com.aliyun.oss.common.comm.IdleConnectionReaper 这个类当中，我们可以看下这个类的源码，可以确认应该就是下面这个静态对象列表。

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192144878-691709235.png)

查看该列表使用的地方，会发现在registerConnectionManager方法内进行填充

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192220209-1341198456.png)

registerConnectionManager方法调用的地方，

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192231104-37449978.png)

 最终定位，果然是在创建OSSClient的时候调用的

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192247039-1619744151.png)

查看OssClient构造函数

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192258165-232127266.png)

我们看下OssClient是如何创建的，为什么connectionManagers列表会持有这么多对象。最终发现在上传图片的过程中，我们每调用一次getOSSClient()就会生成一个OssClient对象，就会在connectionManagers添加一个PoolingHttpClientConnectionManager对象。

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192324859-1582604563.png)

最终问题定位应该就是这里，因为没有及时关闭OssClient导致的内存泄漏，因为不是一瞬间就造成内存暴涨，而是缓慢的增加，所以没有第一时间发现，这也是为什么重启之后就没有报错的原因。

## 三、问题重现

我们本地启动服务，修改一下jvm参数如下（比线上减少了10倍）
-Xms200M -Xmx200M -Xss256K

顺便增加一些测试代码，打印一下当前jvm的内存信息

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192346381-477386337.png)

启动程序，模拟线上上传图片：

随着请求数量增加，可以发现connectionManagers中PoolingHttpClientConnectionManager对象也在不断上涨。

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192404645-1429777643.png)

加大请求次数，通过VirtualGC查看内存分布，也可以发现 老年代一直在上涨，几次GC之后也没有明显下降

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192415295-1510761227.png)

最终在我们还可以看到

1、出现了OOM异常

![image-20240714210908442](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/image-20240714210908442.png)

2、Consul的 Connection pool shut down 也被触发了

![image-20240714210945482](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/image-20240714210945482.png)

综上所有的迹象都说明就是因为OSSClient这个类被一直new，导致内存不断泄漏、最终导致程序OOM使得出现了很多异常。

## 四、问题解决

因为本服务需要提供用户上传图片到阿里云OSS的功能，并且调用频次还算比较高，所以解决方式有两种。
1、每次new OSSClient之后，需要调用shutdown方法，移除PoolingHttpClientConnectionManager对象。

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192528297-733629227.png)

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192539425-981716449.png)

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192544727-1107595461.png)

我们都知道new操作是一个比较耗费资源和性能的操作，而且什么时候shutdown不好控制，所以我觉得这样的方式比较适用于操作比较少的场合。

2、使用单例的OSSClient，看了刚刚的代码OssClient本身也封装了一个PoolingHttpClientConnectionManager对象，说明它本身就是支持链接池的，也就是可以并发访问。

修正为单例之后，执行测试：

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192609793-1470419963.png)

可以看到经过一段时间的请求，整体老年代内存维持不高。

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220228192625694-1336858335.png)

## 五、问题总结

经过这个bug的排查，可以有以下几点收获：
1、遇到OOM问题，仅凭借日志输出可能不能马上定位到问题的本质，最好可以通过dump文件进行分析
2、像HttpClient这种需要建立socket链接的资源，在使用的时候尽量能够池化，避免大量创建对象耗费资源，并且使用后要考虑关闭的问题。

# 记一次因为配置不当导致Hystrix超时问题排查

## 一、背景

最近发现一些Hystrix服务检查报错 ，异常如下：

```java
com.netflix.hystrix.exception.HystrixTimeoutException: null
at com.netflix.hystrix.AbstractCommand$HystrixObservableTimeoutOperator$1$1.run(AbstractCommand.java:1154)
at com.netflix.hystrix.strategy.concurrency.HystrixContextRunnable$1.call(HystrixContextRunnable.java:45)
at com.netflix.hystrix.strategy.concurrency.HystrixContextRunnable$1.call(HystrixContextRunnable.java:41)
at com.hikvision.hikkan.robotdevice.common.config.FeignHystrixConcurrencyStrategyIntellif$WrappedCallable.call(FeignHystrixConcurrencyStrategyIntellif.java:120)
at org.springframework.cloud.sleuth.instrument.async.TraceCallable.call(TraceCallable.java:63)
at com.netflix.hystrix.strategy.concurrency.HystrixContextRunnable.run(HystrixContextRunnable.java:61)
at com.netflix.hystrix.AbstractCommand$HystrixObservableTimeoutOperator$1.tick(AbstractCommand.java:1159)
at com.netflix.hystrix.util.HystrixTimer$1.run(HystrixTimer.java:99)
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
at java.base/java.util.concurrent.FutureTask.runAndReset$$$capture(FutureTask.java:305)
at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java)
at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
at java.base/java.lang.Thread.run(Thread.java:834)
```

## 二、问题分析

根据内容显示明显是 HystrixTimeoutException 请求超时了，但是根据全局记录的整个请求的时间，如下图所示也才3s。

```java
2022-01-17 10:57:21.171 INFO robotdevice [XNIO-2 task-29] [com.xxx.GlobalHandler] - 请求方法:POST XXX 耗时:3118ms
```

而我们在application.yml中配置如下：

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220117113057078-377632062.png)

可知我们预期的超时时间足有25s这么长，但是为什么3s就超时了呢？ 怀疑是配置没有生效。

于是需要找到关于hystrix配置相关的几个类：

HystrixThreadPoolProperties 和 HystrixCommandProperties 这两个类 分别代表设置线程池和Command的属性类。其中HystrixCommandProperties可以设置对应的timeout。

HystrixThreadPoolProperties中的配置

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220117113136962-373815178.png)

HystrixCommandProperties中的配置

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220117113159790-171739819.png)

从上图默认值大概能看出一些端倪，应该是配置没有生效，而使用的默认配置 1s ，这样3s就算超时了。

综上在找一下对应的配置的configkey

```java
//这是配置的赋值语句
this.executionTimeoutInMilliseconds = getProperty(propertyPrefix, key, "execution.isolation.thread.timeoutInMilliseconds",
 builder.getExecutionIsolationThreadTimeoutInMilliseconds(), default_executionTimeoutInMilliseconds);

//propertyPrefix如下是"hystrix"
protected HystrixCommandProperties(HystrixCommandKey key) {
　　this(key, new Setter(), "hystrix");
}

//具体的获取配置并赋值
private static HystrixProperty<Integer> getProperty(String propertyPrefix, HystrixCommandKey key, String instanceProperty,
Integer builderOverrideValue, Integer defaultValue) {
  return forInteger()
     .add(propertyPrefix + ".command." + key.name() + "." + instanceProperty, builderOverrideValue)
     .add(propertyPrefix + ".command.default." + instanceProperty, defaultValue)
     .build();
}
```

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220117113344368-706918102.png)

debug看下这里超时时间设置的是默认的1s。

## 三、问题解决

回头看下我们配置

command放在了threadpool下面，所以找不到对应的配置。应该放在和threadpool同一级

重启服务，可以看到配置生效了。

![img](img/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/785993-20220117113420308-2001544123.png)

## 四、问题总结

配置要严格按照规定！
